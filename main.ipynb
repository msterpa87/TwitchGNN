{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "\n",
    "import warnings\n",
    "from scipy.sparse import SparseEfficiencyWarning\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=SparseEfficiencyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEval():\n",
    "    def __init__(self, print_freq=10):\n",
    "        super(ModelEval, self).__init__()\n",
    "\n",
    "        self.print_freq = print_freq\n",
    "        self.tasks = load_tasks()\n",
    "        self.prior_weights = dict()\n",
    "        self.fisher_matrix = dict()\n",
    "        self.loss_fn = CategoricalCrossentropy()\n",
    "        self.mean_accuracy = GlobalAccuracy()\n",
    "    \n",
    "    def train(self, task_num, epochs=20, continual_learning=False, lambda_=0.1, lr=0.001, verbose=True):\n",
    "        (x, adj, y), (mask_tr, _, _) = self.tasks[task_num]\n",
    "\n",
    "        # initialize model if not already trained\n",
    "        if not continual_learning:\n",
    "            self.model = GNNNodeClassifier()\n",
    "        \n",
    "        accuracy = CategoricalAccuracy()\n",
    "        optim = Adam(learning_rate=lr)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = self.model([x, adj])\n",
    "                loss = self.loss_fn(y[mask_tr], pred[mask_tr])\n",
    "\n",
    "                # if continual learning is enabled add l2 loss wrt previous weights\n",
    "                if continual_learning:\n",
    "                    for i in range(task_num):\n",
    "                        fisher_mat = self.fisher_matrix[i]\n",
    "                        theta = self.prior_weights[i]\n",
    "                        theta_new = flat_tensor_list(self.model.trainable_variables)\n",
    "                        loss += lambda_ * tf.reduce_sum(fisher_mat * ((theta-theta_new)**2))\n",
    "                \n",
    "            accuracy.update_state(y_pred=pred[mask_tr], y_true=y[mask_tr])\n",
    "\n",
    "            gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "            optim.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "            acc = accuracy.result().numpy()\n",
    "            \n",
    "            if i % self.print_freq == 0 and verbose:\n",
    "                print(f\"epoch={i}, train_loss={loss:.3f}, train_acc={acc:.3f}\")\n",
    "        \n",
    "        # after training on current task add accuracy on previous tasks\n",
    "        for i in range(task_num):\n",
    "            (x_prev, a_prev, y_prev), (mask_tr_prev, _, _) = self.tasks[i]\n",
    "            pred_prev = self.model([x_prev, a_prev])\n",
    "            self.mean_accuracy.update_state(y_pred=pred_prev[mask_tr_prev],\n",
    "                                            y_true=y_prev[mask_tr_prev])\n",
    "\n",
    "        # save the flatted list of parameters and fisher matrix of current model\n",
    "        self.prior_weights[task_num] = flat_tensor_list(self.model.trainable_variables)\n",
    "        self.fisher_matrix[task_num] = self.compute_fisher(task_num)\n",
    "    \n",
    "    def compute_fisher(self, task_num):\n",
    "        with tf.GradientTape() as tape:\n",
    "            (x, a, y), (mask_tr, _, _) = self.tasks[task_num]\n",
    "            pred = self.model([x, a])\n",
    "            loss = self.loss_fn(y_pred=pred[mask_tr], y_true=y[mask_tr])\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "\n",
    "        return flat_tensor_list(gradients)**2\n",
    "\n",
    "    def test(self, task_num):\n",
    "        (x, adj, y), (_, _, mask_te) = self.tasks[task_num]\n",
    "\n",
    "        pred = self.model([x, adj])\n",
    "        accuracy = CategoricalAccuracy()\n",
    "        accuracy.update_state(y_pred=pred[mask_te], y_true=y[mask_te])\n",
    "        \n",
    "        return accuracy.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing node features\n",
      "task=0, test_acc=0.759\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "task=1, test_acc=0.859\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n",
      "prev_acc=0.0\n"
     ]
    }
   ],
   "source": [
    "model_eval = ModelEval()\n",
    "n_tasks = 3\n",
    "epochs = 80\n",
    "\n",
    "for i in range(n_tasks):\n",
    "    model_eval.train(i, epochs=epochs, continual_learning=False, verbose=False)\n",
    "    acc = model_eval.test(i)\n",
    "    print(f\"task={i}, test_acc={acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval.mean_accuracy.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing node features\n",
      "Accuracy after training on two three without CL\n",
      "task=0, test_acc=0.482\n",
      "task=1, test_acc=0.248\n",
      "task=2, test_acc=0.854\n"
     ]
    }
   ],
   "source": [
    "model_eval = ModelEval()\n",
    "epochs = 100\n",
    "\n",
    "model_eval.train(0, epochs=epochs, lr=0.01, verbose=False)\n",
    "model_eval.train(1, epochs=epochs, lr=0.01, verbose=False)\n",
    "model_eval.train(2, epochs=epochs, lr=0.01, verbose=False)\n",
    "\n",
    "acc0 = model_eval.test(0)\n",
    "acc1 = model_eval.test(1)\n",
    "acc2 = model_eval.test(2)\n",
    "\n",
    "print(\"Accuracy after training on two three without CL\")\n",
    "print(f\"task=0, test_acc={acc0:.3f}\")\n",
    "print(f\"task=1, test_acc={acc1:.3f}\")\n",
    "print(f\"task=2, test_acc={acc2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing node features\n",
      "epochs=100, lr=0.001, lambda_=0.1, avg_acc=0.548\n",
      "epochs=100, lr=0.01, lambda_=0.1, avg_acc=0.558\n",
      "epochs=100, lr=0.001, lambda_=1, avg_acc=0.556\n",
      "epochs=100, lr=0.01, lambda_=1, avg_acc=0.559\n",
      "epochs=100, lr=0.001, lambda_=10, avg_acc=0.548\n",
      "epochs=100, lr=0.01, lambda_=10, avg_acc=0.551\n",
      "epochs=100, lr=0.001, lambda_=100, avg_acc=0.551\n",
      "epochs=100, lr=0.01, lambda_=100, avg_acc=0.547\n",
      "epochs=200, lr=0.001, lambda_=0.1, avg_acc=0.556\n",
      "epochs=200, lr=0.01, lambda_=0.1, avg_acc=0.540\n",
      "epochs=200, lr=0.001, lambda_=1, avg_acc=0.548\n",
      "epochs=200, lr=0.01, lambda_=1, avg_acc=0.555\n",
      "epochs=200, lr=0.001, lambda_=10, avg_acc=0.551\n",
      "epochs=200, lr=0.01, lambda_=10, avg_acc=0.551\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-d7efe9ba5cf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel_eval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmodel_eval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontinual_learning\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mmodel_eval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontinual_learning\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0macc0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_eval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-855acee7be16>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, task_num, epochs, continual_learning, lambda_, lr, verbose)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask_tr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask_tr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1082\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1085\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_BiasAddGrad\u001b[1;34m(op, received_grad)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[0mdata_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m   return (received_grad,\n\u001b[1;32m--> 347\u001b[1;33m           gen_nn_ops.bias_add_grad(\n\u001b[0m\u001b[0;32m    348\u001b[0m               out_backprop=received_grad, data_format=data_format))\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add_grad\u001b[1;34m(out_backprop, data_format, name)\u001b[0m\n\u001b[0;32m    746\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m    749\u001b[0m         _ctx, \"BiasAddGrad\", name, out_backprop, \"data_format\", data_format)\n\u001b[0;32m    750\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_eval = ModelEval(print_freq=25)\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = {'epochs': [100, 200], 'lambda_': [0.1, 1, 10, 100], 'lr': [0.001, 0.01]}\n",
    "for grid in list(ParameterGrid(param_grid)):\n",
    "\n",
    "    model_eval.train(0, verbose=False, **grid)\n",
    "    model_eval.train(1, continual_learning=True, verbose=False, **grid)\n",
    "    model_eval.train(2, continual_learning=True, verbose=False, **grid)\n",
    "\n",
    "    acc0 = model_eval.test(0)\n",
    "    acc1 = model_eval.test(1)\n",
    "    acc2 = model_eval.test(2)\n",
    "\n",
    "    #print(\"Accuracy after training on two tasks with CL and EWC\")\n",
    "    #print(f\"task=0, test_acc={acc0:.3f}\")\n",
    "    #print(f\"task=1, test_acc={acc1:.3f}\")\n",
    "    #print(f\"task=2, test_acc={acc2:.3f}\")\n",
    "    avg_acc = np.mean([acc0, acc1, acc2])\n",
    "    print(f\"epochs={grid['epochs']}, lr={grid['lr']}, lambda_={grid['lambda_']}, avg_acc={avg_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a0cdf5044e235b24d1fc042fb09c724f9ff16da5a82a5cb2270a9975034c9c0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
