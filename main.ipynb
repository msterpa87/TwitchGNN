{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "\n",
    "import warnings\n",
    "from scipy.sparse import SparseEfficiencyWarning\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=SparseEfficiencyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEval():\n",
    "    def __init__(self, print_freq=10, continual_learning=False,\n",
    "                 lambda_=0.1, verbose=True, print_previous_val=False):\n",
    "        super(ModelEval, self).__init__()\n",
    "\n",
    "        self.print_freq = print_freq\n",
    "        self.tasks = load_tasks_new()\n",
    "        self.prior_weights = dict()\n",
    "        self.fisher_matrix = dict()\n",
    "        self.loss_fn = CategoricalCrossentropy()\n",
    "        self.mean_accuracy = GlobalAccuracy()\n",
    "        self.backward_transfer = BackwardTransfer(len(self.tasks))\n",
    "        self.continual_learning = continual_learning\n",
    "        self.lambda_ = lambda_\n",
    "        self.model = None\n",
    "        self.verbose = verbose\n",
    "        self.print_previous_val = print_previous_val\n",
    "    \n",
    "    def train(self, task_num, epochs=100, lr=0.001):\n",
    "        (x, adj, y), (mask_tr, _, _) = self.tasks[task_num]\n",
    "\n",
    "        # initialize model if not already trained\n",
    "        if not self.continual_learning or self.model is None:\n",
    "            self.model = GNNNodeClassifier()\n",
    "        \n",
    "        accuracy = CategoricalAccuracy()\n",
    "        optim = Adam(learning_rate=self.lr)\n",
    "        \n",
    "        for i in range(epochs+1):\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = self.model([x, adj])\n",
    "                loss = self.loss_fn(y[mask_tr], pred[mask_tr])\n",
    "\n",
    "                # if continual learning is enabled add l2 loss wrt previous weights\n",
    "                if self.continual_learning:\n",
    "                    for j in range(task_num):\n",
    "                        fisher_mat = self.fisher_matrix[j]\n",
    "                        theta = self.prior_weights[j]\n",
    "                        theta_new = flat_tensor_list(self.model.trainable_variables)\n",
    "                        penalty = self.lambda_ * tf.reduce_sum(fisher_mat * ((theta-theta_new)**2))\n",
    "                        loss += penalty\n",
    "                \n",
    "            accuracy.update_state(y_pred=pred[mask_tr], y_true=y[mask_tr])\n",
    "\n",
    "            gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "            optim.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "            train_acc = accuracy.result().numpy()\n",
    "            \n",
    "            if i % self.print_freq == 0 and self.verbose:\n",
    "                val_acc = self.test(task_num, validation=True, print_acc=False)\n",
    "                s = f\"epoch={i}, train_loss={loss:.3f}, train_acc={train_acc:.3f}, val_acc={val_acc:.3f}\"\n",
    "                \n",
    "                if self.print_previous_val:\n",
    "                    for j in range(task_num):\n",
    "                        prev_acc = self.test(j, validation=True, print_acc=False)\n",
    "                        s += f\", task[{j}]_val_acc={prev_acc:.3f}\"\n",
    "                \n",
    "                print(s)\n",
    "        \n",
    "        # after training on current task add accuracy on previous tasks\n",
    "        for i in range(task_num+1):\n",
    "            (x_i, a_i, y_i), (_, _, mask_te_i) = self.tasks[i]\n",
    "            y_pred_i = self.model([x_i, a_i])[mask_te_i]\n",
    "            y_true_i = y_i[mask_te_i]\n",
    "\n",
    "            self.mean_accuracy.update_state(y_pred=y_pred_i, y_true=y_true_i)\n",
    "            \n",
    "            self.backward_transfer.update_state(task_i=i, task_j=task_num,\n",
    "                                                y_pred=y_pred_i, y_true=y_true_i)\n",
    "\n",
    "        # save the flatted list of parameters and fisher matrix of current model\n",
    "        self.prior_weights[task_num] = flat_tensor_list(self.model.trainable_variables)\n",
    "        self.fisher_matrix[task_num] = self.compute_fisher(task_num)\n",
    "    \n",
    "    def compute_fisher(self, task_num):\n",
    "        with tf.GradientTape() as tape:\n",
    "            (x, a, y), (mask_tr, _, _) = self.tasks[task_num]\n",
    "            pred = self.model([x, a])\n",
    "            loss = self.loss_fn(y_pred=pred[mask_tr], y_true=y[mask_tr])\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "\n",
    "        return flat_tensor_list(gradients)**2\n",
    "\n",
    "    def test(self, task_num, validation=False, print_acc=True):\n",
    "        (x, adj, y), (_, mask_va, mask_te) = self.tasks[task_num]\n",
    "\n",
    "        mask = [mask_te, mask_va][validation]\n",
    "\n",
    "        pred = self.model([x, adj])\n",
    "        accuracy = CategoricalAccuracy()\n",
    "        accuracy.update_state(y_pred=pred[mask], y_true=y[mask])\n",
    "        val = accuracy.result().numpy()\n",
    "        \n",
    "        if print_acc:\n",
    "            print(f\"accuracy task {task_num} = {val:.3f}\")\n",
    "        \n",
    "        return val\n",
    "    \n",
    "    def print_mean_accuracy(self):\n",
    "        acc = self.mean_accuracy.result().numpy()\n",
    "        print(f\"mean_accuracy = {acc:.3f}\")\n",
    "    \n",
    "    def print_backward_transfer(self):\n",
    "        acc = self.backward_transfer.result().numpy()\n",
    "        print(f\"backward_transfer = {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing node features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spektral\\datasets\\citation.py:194: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n",
      "C:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\sparse\\_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after training on the task:\n",
      "accuracy task 0 = 0.838\n",
      "accuracy task 1 = 0.815\n",
      "accuracy task 2 = 0.829\n",
      "mean_accuracy = 0.640\n"
     ]
    }
   ],
   "source": [
    "n_tasks = 3\n",
    "\n",
    "model_eval = ModelEval(continual_learning=False, verbose=False)\n",
    "\n",
    "print(\"Accuracy after training on the task:\")\n",
    "for i in range(n_tasks):\n",
    "    model_eval.train(i)\n",
    "    model_eval.test(i)\n",
    "\n",
    "model_eval.print_mean_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing node features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spektral\\datasets\\citation.py:194: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n",
      "C:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\sparse\\_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after training on all 3 tasks:\n",
      "accuracy task 0 = 0.359\n",
      "accuracy task 1 = 0.450\n",
      "accuracy task 2 = 0.829\n",
      "mean_accuracy = 0.641\n",
      "backward_transfer = -1.120\n"
     ]
    }
   ],
   "source": [
    "model_eval = ModelEval(verbose=False)\n",
    "\n",
    "model_eval.train(0, )\n",
    "model_eval.train(1)\n",
    "model_eval.train(2)\n",
    "\n",
    "print(\"Accuracy after training on all 3 tasks:\")\n",
    "model_eval.test(0)\n",
    "model_eval.test(1)\n",
    "model_eval.test(2)\n",
    "\n",
    "model_eval.print_mean_accuracy()\n",
    "model_eval.print_backward_transfer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing node features\n",
      "Training on task 0\n",
      "epoch=0, train_loss=1.099, train_acc=0.424, val_acc=0.755\n",
      "epoch=50, train_loss=0.657, train_acc=0.739, val_acc=0.755\n",
      "epoch=100, train_loss=0.319, train_acc=0.771, val_acc=0.836\n",
      "\n",
      "Training on task 1\n",
      "epoch=0, train_loss=1.365, train_acc=0.563, val_acc=0.537, task[0]_val_acc=0.829\n",
      "epoch=50, train_loss=0.557, train_acc=0.627, val_acc=0.785, task[0]_val_acc=0.621\n",
      "epoch=100, train_loss=0.338, train_acc=0.742, val_acc=0.836, task[0]_val_acc=0.561\n",
      "\n",
      "Training on task 2\n",
      "epoch=0, train_loss=1.905, train_acc=0.409, val_acc=0.435, task[0]_val_acc=0.532, task[1]_val_acc=0.824\n",
      "epoch=50, train_loss=0.477, train_acc=0.694, val_acc=0.853, task[0]_val_acc=0.383, task[1]_val_acc=0.465\n",
      "epoch=100, train_loss=0.291, train_acc=0.798, val_acc=0.862, task[0]_val_acc=0.377, task[1]_val_acc=0.448\n",
      "\n",
      "Testing on task 0\n",
      "accuracy task 0 = 0.365\n",
      "\n",
      "Testing on task 1\n",
      "accuracy task 1 = 0.465\n",
      "\n",
      "Testing on task 2\n",
      "accuracy task 2 = 0.839\n",
      "\n",
      "mean_accuracy = 0.652\n",
      "backward_transfer = -1.108\n"
     ]
    }
   ],
   "source": [
    "model_eval = ModelEval(continual_learning=True, lambda_=1e4,\n",
    "                       verbose=True, print_freq=50, print_previous_val=True)\n",
    "\n",
    "num_tasks = 3\n",
    "\n",
    "for i in range(num_tasks):\n",
    "    print(f\"Training on task {i}\")\n",
    "    model_eval.train(i)\n",
    "    print()\n",
    "\n",
    "for i in range(num_tasks):\n",
    "    print(f\"Testing on task {i}\")\n",
    "    model_eval.test(i)\n",
    "    print()\n",
    "\n",
    "model_eval.print_mean_accuracy()\n",
    "model_eval.print_backward_transfer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a0cdf5044e235b24d1fc042fb09c724f9ff16da5a82a5cb2270a9975034c9c0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
