{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from dataloader import Twitch\n",
    "import tensorflow as tf\n",
    "from spektral.layers import GCNConv\n",
    "from tensorflow.keras import layers\n",
    "from utils import *\n",
    "from spektral.data.loaders import SingleLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spektral.layers import GCNConv\n",
    "from spektral.models.gcn import GCN\n",
    "from spektral.transforms import LayerPreprocess\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_FFN(hidden_units, dropout_rate, name=None):\n",
    "    fnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(fnn_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNNodeClassifier(tf.keras.Model):\n",
    "    def __init__(self, num_classes, hidden_channels, out_channels, dropout_rate=0.2):\n",
    "        self.conv1 = GCNConv(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels)\n",
    "        #self.ffn = build_FFN(hidden_channels, dropout_rate)\n",
    "        self.fn1 = layers.Dense(num_classes)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x, adj = inputs\n",
    "        \n",
    "        x = self.conv1([x, adj])\n",
    "        x = self.conv2([x, adj])\n",
    "        x = self.fn1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_weights(mask):\n",
    "    return mask.astype(np.float32) / np.count_nonzero(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Twitch(\"DE\", normalize_x=True, transforms=[LayerPreprocess(GCNConv)])\n",
    "\n",
    "weights_tr, weights_va, weights_te = (\n",
    "    mask_to_weights(mask)\n",
    "    for mask in (dataset.mask_tr, dataset.mask_va, dataset.mask_te)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_tr = SingleLoader(dataset, sample_weights=weights_tr)\n",
    "loader_va = SingleLoader(dataset, sample_weights=weights_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(n_labels=dataset.n_labels, n_input_channels=dataset.n_node_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=BinaryCrossentropy(),\n",
    "    weighted_metrics=[\"acc\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0041 - acc: 0.6046 - val_loss: 0.0038 - val_acc: 0.6042\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0038 - acc: 0.6046 - val_loss: 0.0035 - val_acc: 0.6042\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0035 - acc: 0.6046 - val_loss: 0.0032 - val_acc: 0.6042\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0032 - acc: 0.6046 - val_loss: 0.0030 - val_acc: 0.6042\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0030 - acc: 0.6046 - val_loss: 0.0027 - val_acc: 0.6042\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0027 - acc: 0.6046 - val_loss: 0.0025 - val_acc: 0.6042\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0025 - acc: 0.6046 - val_loss: 0.0023 - val_acc: 0.6042\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0023 - acc: 0.6046 - val_loss: 0.0021 - val_acc: 0.6042\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0021 - acc: 0.6046 - val_loss: 0.0019 - val_acc: 0.6042\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0019 - acc: 0.6046 - val_loss: 0.0017 - val_acc: 0.6042\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.0017 - acc: 0.6046 - val_loss: 0.0016 - val_acc: 0.6042\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0016 - acc: 0.6046 - val_loss: 0.0014 - val_acc: 0.6042\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0014 - acc: 0.6046 - val_loss: 0.0013 - val_acc: 0.6042\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.0013 - acc: 0.6046 - val_loss: 0.0012 - val_acc: 0.6042\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0012 - acc: 0.6046 - val_loss: 0.0011 - val_acc: 0.6042\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0011 - acc: 0.6046 - val_loss: 9.8081e-04 - val_acc: 0.6042\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 9.8082e-04 - acc: 0.6046 - val_loss: 8.8696e-04 - val_acc: 0.6042\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 8.8697e-04 - acc: 0.6046 - val_loss: 8.0132e-04 - val_acc: 0.6042\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 8.0133e-04 - acc: 0.6046 - val_loss: 7.2333e-04 - val_acc: 0.6042\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.2335e-04 - acc: 0.6046 - val_loss: 6.5247e-04 - val_acc: 0.6042\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 6.5248e-04 - acc: 0.6046 - val_loss: 5.8823e-04 - val_acc: 0.6042\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 5.8823e-04 - acc: 0.6046 - val_loss: 5.3011e-04 - val_acc: 0.6042\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 5.3012e-04 - acc: 0.6046 - val_loss: 4.7765e-04 - val_acc: 0.6042\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 4.7767e-04 - acc: 0.6046 - val_loss: 4.3042e-04 - val_acc: 0.6042\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 4.3043e-04 - acc: 0.6046 - val_loss: 3.8799e-04 - val_acc: 0.6042\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 3.8801e-04 - acc: 0.6046 - val_loss: 3.4997e-04 - val_acc: 0.6042\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 3.4997e-04 - acc: 0.6046 - val_loss: 3.1598e-04 - val_acc: 0.6042\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 3.1600e-04 - acc: 0.6046 - val_loss: 2.8568e-04 - val_acc: 0.6042\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.8569e-04 - acc: 0.6046 - val_loss: 2.5873e-04 - val_acc: 0.6042\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.5874e-04 - acc: 0.6046 - val_loss: 2.3482e-04 - val_acc: 0.6042\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.3483e-04 - acc: 0.6046 - val_loss: 2.1368e-04 - val_acc: 0.6042\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1368e-04 - acc: 0.6046 - val_loss: 1.9503e-04 - val_acc: 0.6042\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 1.9505e-04 - acc: 0.6046 - val_loss: 1.7862e-04 - val_acc: 0.6042\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 1.7863e-04 - acc: 0.6046 - val_loss: 1.6424e-04 - val_acc: 0.6042\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 1.6425e-04 - acc: 0.6046 - val_loss: 1.5167e-04 - val_acc: 0.6042\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 1.5167e-04 - acc: 0.6046 - val_loss: 1.4071e-04 - val_acc: 0.6042\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 1.4070e-04 - acc: 0.6046 - val_loss: 1.3118e-04 - val_acc: 0.6042\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 1.3118e-04 - acc: 0.6046 - val_loss: 1.2294e-04 - val_acc: 0.6042\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 1.2295e-04 - acc: 0.6046 - val_loss: 1.1582e-04 - val_acc: 0.6042\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 1.1583e-04 - acc: 0.6046 - val_loss: 1.0970e-04 - val_acc: 0.6042\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 1.0970e-04 - acc: 0.6046 - val_loss: 1.0445e-04 - val_acc: 0.6042\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 1.0445e-04 - acc: 0.6046 - val_loss: 9.9967e-05 - val_acc: 0.6042\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 9.9988e-05 - acc: 0.6046 - val_loss: 9.6150e-05 - val_acc: 0.6042\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 9.6143e-05 - acc: 0.6046 - val_loss: 9.2910e-05 - val_acc: 0.6042\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 9.2916e-05 - acc: 0.6046 - val_loss: 9.0170e-05 - val_acc: 0.6042\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 9.0172e-05 - acc: 0.6046 - val_loss: 8.7861e-05 - val_acc: 0.6042\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 8.7854e-05 - acc: 0.6046 - val_loss: 8.5919e-05 - val_acc: 0.6042\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 8.5921e-05 - acc: 0.6046 - val_loss: 8.4292e-05 - val_acc: 0.6042\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 8.4298e-05 - acc: 0.6046 - val_loss: 8.2930e-05 - val_acc: 0.6042\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 8.2943e-05 - acc: 0.6046 - val_loss: 8.1793e-05 - val_acc: 0.6042\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 8.1803e-05 - acc: 0.6046 - val_loss: 8.0843e-05 - val_acc: 0.6042\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 8.0844e-05 - acc: 0.6046 - val_loss: 8.0049e-05 - val_acc: 0.6042\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 8.0047e-05 - acc: 0.6046 - val_loss: 7.9385e-05 - val_acc: 0.6042\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 7.9382e-05 - acc: 0.6046 - val_loss: 7.8826e-05 - val_acc: 0.6042\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 7.8834e-05 - acc: 0.6046 - val_loss: 7.8354e-05 - val_acc: 0.6042\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 7.8357e-05 - acc: 0.6046 - val_loss: 7.7952e-05 - val_acc: 0.6042\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.7957e-05 - acc: 0.6046 - val_loss: 7.7605e-05 - val_acc: 0.6042\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 7.7613e-05 - acc: 0.6046 - val_loss: 7.7303e-05 - val_acc: 0.6042\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 7.7300e-05 - acc: 0.6046 - val_loss: 7.7035e-05 - val_acc: 0.6042\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 7.7035e-05 - acc: 0.6046 - val_loss: 7.6794e-05 - val_acc: 0.6042\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 7.6800e-05 - acc: 0.6046 - val_loss: 7.6575e-05 - val_acc: 0.6042\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 7.6575e-05 - acc: 0.6046 - val_loss: 7.6371e-05 - val_acc: 0.6042\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.6374e-05 - acc: 0.6046 - val_loss: 7.6180e-05 - val_acc: 0.6042\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 7.6175e-05 - acc: 0.6046 - val_loss: 7.5998e-05 - val_acc: 0.6042\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.6011e-05 - acc: 0.6046 - val_loss: 7.5824e-05 - val_acc: 0.6042\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 7.5819e-05 - acc: 0.6046 - val_loss: 7.5656e-05 - val_acc: 0.6042\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 7.5658e-05 - acc: 0.6046 - val_loss: 7.5493e-05 - val_acc: 0.6042\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.5492e-05 - acc: 0.6046 - val_loss: 7.5334e-05 - val_acc: 0.6042\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 7.5346e-05 - acc: 0.6046 - val_loss: 7.5179e-05 - val_acc: 0.6042\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.5183e-05 - acc: 0.6046 - val_loss: 7.5029e-05 - val_acc: 0.6042\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 7.5039e-05 - acc: 0.6046 - val_loss: 7.4882e-05 - val_acc: 0.6042\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 7.4878e-05 - acc: 0.6046 - val_loss: 7.4741e-05 - val_acc: 0.6042\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.4738e-05 - acc: 0.6046 - val_loss: 7.4603e-05 - val_acc: 0.6042\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 7.4600e-05 - acc: 0.6046 - val_loss: 7.4471e-05 - val_acc: 0.6042\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.4475e-05 - acc: 0.6046 - val_loss: 7.4344e-05 - val_acc: 0.6042\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 7.4357e-05 - acc: 0.6046 - val_loss: 7.4222e-05 - val_acc: 0.6042\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 7.4216e-05 - acc: 0.6046 - val_loss: 7.4107e-05 - val_acc: 0.6042\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 7.4117e-05 - acc: 0.6046 - val_loss: 7.3997e-05 - val_acc: 0.6042\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 7.3994e-05 - acc: 0.6046 - val_loss: 7.3893e-05 - val_acc: 0.6042\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 7.3905e-05 - acc: 0.6046 - val_loss: 7.3795e-05 - val_acc: 0.6042\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.3797e-05 - acc: 0.6046 - val_loss: 7.3704e-05 - val_acc: 0.6042\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.3714e-05 - acc: 0.6046 - val_loss: 7.3619e-05 - val_acc: 0.6042\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 7.3618e-05 - acc: 0.6046 - val_loss: 7.3540e-05 - val_acc: 0.6042\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 7.3553e-05 - acc: 0.6046 - val_loss: 7.3467e-05 - val_acc: 0.6042\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.3465e-05 - acc: 0.6046 - val_loss: 7.3400e-05 - val_acc: 0.6042\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 7.3405e-05 - acc: 0.6046 - val_loss: 7.3338e-05 - val_acc: 0.6042\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.3356e-05 - acc: 0.6046 - val_loss: 7.3282e-05 - val_acc: 0.6042\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 7.3288e-05 - acc: 0.6046 - val_loss: 7.3231e-05 - val_acc: 0.6042\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 7.3240e-05 - acc: 0.6046 - val_loss: 7.3185e-05 - val_acc: 0.6042\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 7.3184e-05 - acc: 0.6046 - val_loss: 7.3144e-05 - val_acc: 0.6042\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.3145e-05 - acc: 0.6046 - val_loss: 7.3106e-05 - val_acc: 0.6042\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 7.3110e-05 - acc: 0.6046 - val_loss: 7.3073e-05 - val_acc: 0.6042\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 7.3072e-05 - acc: 0.6046 - val_loss: 7.3044e-05 - val_acc: 0.6042\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 7.3048e-05 - acc: 0.6046 - val_loss: 7.3018e-05 - val_acc: 0.6042\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 7.3031e-05 - acc: 0.6046 - val_loss: 7.2994e-05 - val_acc: 0.6042\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.2999e-05 - acc: 0.6046 - val_loss: 7.2974e-05 - val_acc: 0.6042\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 7.2973e-05 - acc: 0.6046 - val_loss: 7.2956e-05 - val_acc: 0.6042\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 7.2951e-05 - acc: 0.6046 - val_loss: 7.2941e-05 - val_acc: 0.6042\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 7.2946e-05 - acc: 0.6046 - val_loss: 7.2928e-05 - val_acc: 0.6042\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 7.2933e-05 - acc: 0.6046 - val_loss: 7.2916e-05 - val_acc: 0.6042\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 7.2920e-05 - acc: 0.6046 - val_loss: 7.2906e-05 - val_acc: 0.6042\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 7.2913e-05 - acc: 0.6046 - val_loss: 7.2897e-05 - val_acc: 0.6042\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 7.2900e-05 - acc: 0.6046 - val_loss: 7.2890e-05 - val_acc: 0.6042\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 7.2886e-05 - acc: 0.6046 - val_loss: 7.2884e-05 - val_acc: 0.6042\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 7.2885e-05 - acc: 0.6046 - val_loss: 7.2878e-05 - val_acc: 0.6042\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 7.2890e-05 - acc: 0.6046 - val_loss: 7.2873e-05 - val_acc: 0.6042\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 7.2883e-05 - acc: 0.6046 - val_loss: 7.2870e-05 - val_acc: 0.6042\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 7.2865e-05 - acc: 0.6046 - val_loss: 7.2866e-05 - val_acc: 0.6042\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 7.2880e-05 - acc: 0.6046 - val_loss: 7.2863e-05 - val_acc: 0.6042\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.2862e-05 - acc: 0.6046 - val_loss: 7.2861e-05 - val_acc: 0.6042\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 7.2851e-05 - acc: 0.6046 - val_loss: 7.2858e-05 - val_acc: 0.6042\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 7.2860e-05 - acc: 0.6046 - val_loss: 7.2856e-05 - val_acc: 0.6042\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 7.2866e-05 - acc: 0.6046 - val_loss: 7.2855e-05 - val_acc: 0.6042\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.2864e-05 - acc: 0.6046 - val_loss: 7.2853e-05 - val_acc: 0.6042\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 7.2854e-05 - acc: 0.6046 - val_loss: 7.2852e-05 - val_acc: 0.6042\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 7.2852e-05 - acc: 0.6046 - val_loss: 7.2850e-05 - val_acc: 0.6042\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 7.2862e-05 - acc: 0.6046 - val_loss: 7.2849e-05 - val_acc: 0.6042\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 7.2863e-05 - acc: 0.6046 - val_loss: 7.2848e-05 - val_acc: 0.6042\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 7.2847e-05 - acc: 0.6046 - val_loss: 7.2847e-05 - val_acc: 0.6042\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 7.2851e-05 - acc: 0.6046 - val_loss: 7.2846e-05 - val_acc: 0.6042\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 7.2853e-05 - acc: 0.6046 - val_loss: 7.2845e-05 - val_acc: 0.6042\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 7.2861e-05 - acc: 0.6046 - val_loss: 7.2844e-05 - val_acc: 0.6042\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.2853e-05 - acc: 0.6046"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-20fe15a62965>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpatience\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m model.fit(\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mloader_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloader_va\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1250\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1252\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1253\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1536\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1537\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1538\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1539\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    947\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 1e-2\n",
    "epochs = 200\n",
    "patience = 10\n",
    "\n",
    "model.fit(\n",
    "    loader_tr.load(),\n",
    "    steps_per_epoch=loader_va.steps_per_epoch,\n",
    "    validation_data=loader_va.load(),\n",
    "    validation_steps=loader_va.steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    callbacks=[EarlyStopping(patience=patience, restore_best_weights=True)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model.\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 7.2833e-05 - acc: 0.6047\n",
      "Done.\n",
      "Test loss: 7.283292507054284e-05\n",
      "Test accuracy: 0.604736864566803\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model.\")\n",
    "loader_te = SingleLoader(dataset, sample_weights=weights_te)\n",
    "eval_results = model.evaluate(loader_te.load(), steps=loader_te.steps_per_epoch)\n",
    "print(\"Done.\\n\" \"Test loss: {}\\n\" \"Test accuracy: {}\".format(*eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a0cdf5044e235b24d1fc042fb09c724f9ff16da5a82a5cb2270a9975034c9c0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
